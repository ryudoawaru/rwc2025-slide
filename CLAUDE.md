# CLAUDE.md

## RubyWorld Conference 2025 Presentation

This project aims to generate Marp format slides for presentation at RubyWorld Conference 2025 on November 6th. A Chinese version will also be prepared for RubyJam on October 28th.

## Slide Files

- **Main File**: `rubyworld-2025-taigi-parser.md`
- **Theme**: `5xruby.css`
- **Presentation Time**: 15 minutes (pure presentation, no Q&A)

## Project Goals

- Generate Marp format slides in Japanese for RubyWorld Conference 2025 (15-minute presentation) with speaker notes
- Target file: rubyworld-2025-taigi-parser.md
- All images stored in `images/`

## Image Generation

Reference styles used:

Simplified ICON style:

```
Flat design icon: A simple maze from top view with a confused person holding a "RFP è¦æ±‚ä»•æ§˜æ›¸" document in the center. Red X marks on wrong paths. Golden orange and cream color scheme. Minimalist style.
```

General style:
```
A flat design illustration showing two contrasting scenes split vertically. Left side: a developer sitting at a computer with code on
  the screen, looking stressed and looking at a clock showing limited time. Right side: the same developer in a business suit at a
  networking event, shaking hands with multiple government officials, holding a stack of business cards, with a calendar in background
  showing "å–¶æ¥­æ´»å‹•: 80%" (Sales Activities: 80%) in Japanese. The developer's laptop is closed and pushed to the side. The contrast
  shows time being taken away from development work. Color scheme: warm golden orange/amber for suits and main elements, cream/beige
  background, brown outlines. Soft pastel illustration style, warm and friendly colors, clean lines. Same color palette throughout both
  scenes.
```

Overall text should be in Japanese.

## Presentation Structure Overview (Latest Version 2025-10-19)

### Main Sections

**Total Pages**: ~30+ pages (including detailed technical pages)
**Presentation Time**: 15 minutes

1. **Opening** (Pages 1-5)
   - Page 1: Title Page
   - Page 2: Self Introduction
   - Page 3: 10-Year Story with RubyCity Matsue (MOU)
   - Page 4: About 5xRuby
   - Page 5: 5xRuby's Business

2. **Act 1: The Story of No Bidders** (Pages 6-8)
   - Page 6: Peculiarities of Taiwan Government Projects
   - Page 7: Lessons from 8 Consecutive Losses
   - Page 8: Truth After Winning Bid (Word segmentation too complex, nobody dared)

3. **Act 2: What is POJ (Taiwanese Romanization)?** (Pages 9-11)
   - Page 9: What is POJ?
   - Page 10: Japanese and Taiwanese Writing Systems
   - Page 11: Real Example of Word Segmentation Alignment Processing

4. **Act 3: Word Segmentation Alignment Implementation** (Pages 12-17)
   - Page 12: Pattern Rule Systematization (65+ patterns)
   - Pages 13-16: Implementation Flow
     - Step 1: Kanji Splitting Processing
     - Step 2: POJ Splitting Processing
     - Step 3: Alignment Logic
     - Step 4: Array Construction and Validation

5. **Act 4: Encounter with Parser** (Pages 18-27) â­ Core Section
   - Page 18: Inspiration from RubyConf Taiwan x COSCUP 2025
   - Page 19: Insights from Kaneko's Talk
   - Page 20: RomanParser - Parslet Implementation
   - Pages 21-23: 3-Phase Details
     - Phase 1: Lexical Analysis
     - Phase 2: Syntax Analysis
     - Phase 3: Semantic Analysis
   - Page 24: Comparison with Ruby Parser
   - **Page 25: Why Kanji Doesn't Need a Parser?** ğŸ†•
     - POJ syllable count = Kanji character count
     - 1:1 automatic alignment principle

6. **Act 5: Ruby's Advantages** (Pages 26-28)
   - Page 26: Ruby's 3 Key Advantages
   - Page 27: Project Results

7. **Conclusion** (Pages 28-30)
   - Page 28: Summary
   - Page 29: Thank You

### Key Page Markers

- â­ **Page 25**: New core insight page explaining why Kanji doesn't need independent Parser
- ğŸ”„ **Deleted**: Original Page 28 "3-Phase Analysis Details" (duplicate)
- ğŸ”„ **Deleted**: Original Page 29 "Compiler Theory Application" (abstract and duplicate)

## Important Revision Log

### 2025-10-19: Presentation Structure Reorganization and Optimization ğŸ¯

#### Removed Redundant Pages
1. **Deleted Original Page 28 "3-Phase Analysis Details"**
   - **Reason**: Duplicates Phase 1/2/3 detail pages
   - **Location**: Lines 2017-2066
   - **Impact**: Presentation more focused, avoiding repetition

2. **Deleted Original Page 29 "Compiler Theory Application"**
   - **Reason**: Only repeats abstract concepts, no new information
   - **Content**: Only explains Parser knowledge can apply to NLP, already proven in Page 27 comparison
   - **Impact**: After removal, flows directly into Ruby advantages section, better logic

#### Added Key Page
3. **Added Page 28 "Why Kanji Doesn't Need a Parser?"** â­
   - **Position**: After Page 27 "Comparison with Ruby Parser"
   - **Core Concept**: POJ syllable count = Kanji character count
   - **Content Structure**:
     - Left column: POJ Parser output and syllable counting
       - `"suÃ -loÌh".split('-').size # => 2`
       - Hyphen = syllable separator
     - Right column: Kanji automatic alignment
       - 2 syllables â†’ Take 2 Kanji "ç´²è½"
       - 3 syllables â†’ Take 3 Kanji "æ–°ç«¹å¸‚"
   - **Conclusion**:
     - âœ… Kanji split using POJ Parser's syllable info
     - âœ… Kanji Parser unnecessary
     - âœ… Achieved through simple character counting
   - **Speaker Notes Key Points**:
     - Explain 1 syllable = 1 Kanji correspondence
     - Show concrete steps from POJ syllable count to Kanji count
     - Analogy to Ruby Parser principle: complex structure parsed first, simple structure naturally corresponds
     - Emphasize universality of Compiler theory

#### Logic Improvements
- **Before**: Comparison â†’ 3-Phase Details â†’ Abstract Theory â†’ Ruby Advantages
- **After**: Comparison â†’ **Why Kanji Doesn't Need Parser** â†’ Ruby Advantages
- **Improvements**:
  - Reduced repetition
  - Added key insight (Parser one-way dependency)
  - More coherent logic: Parser comparison â†’ Deep understanding (why Kanji doesn't need) â†’ Technical advantages

### 2025-10-12: Terminology Unification
- **Change**: `æ‹†å­—ï¼ˆåˆ†è©ï¼‰` â†’ `åˆ†è©ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆå‡¦ç†` (Word Segmentation Alignment Processing)
- **Location**: Line 295
- **Reason**: Use more precise technical terminology

### 2025-10-12: Page 12a Major Correction
- **Issue**: Originally listed 4 hyphen handling types, but type 4 (`æ–‡è„ˆä¾å­˜ã®åˆ†é›¢`) not actually implemented
- **Fix**: Changed to 3 handling types
  1. Intra-word hyphens (preserve)
  2. Double hyphen (boundary marker)
  3. Prefix hyphen (inter-word pause)
- **New Example**: `jiÌt--sÃ®` (date-time)
  ```
  Kanji: "æ—¥æ™‚æ–·æ–·ä»”"
  POJ:   "jiÌt--sÃ® tuÄn-tuÄn-Ã¡"
  ```
- **Key Findings**:
  - Final split uses space (`split(/\s/)`), not hyphen
  - `--` handled on KANJI side (line 68), but commented out on ROMAN side (lines 48-49)
  - Prefix hyphen specially handled in `roman_kanji_array` method (lines 146-149)

## Scale Classes Usage

To prevent content overflow into footer area, this presentation uses custom scale utility classes:

```css
section.scale-95 { font-size: 95%; }
section.scale-90 { font-size: 90%; }
section.scale-85 { font-size: 85%; }
section.scale-80 { font-size: 80%; }
section.scale-75 { font-size: 75%; }
section.scale-70 { font-size: 70%; }
section.scale-65 { font-size: 65%; }
```

### Current Usage
- **Pages 12a-12d**: Using `scale-75` (75%)
  - Reason: Technical detail pages, dense content
  - Contains: Code examples, demonstrations, explanatory text

### When to Use Scale Classes
1. Pages rich in technical details
2. Pages with multiple code blocks
3. Pages showing input/output examples simultaneously
4. When content approaches or exceeds footer area

## Code Source Reference

All code examples in this presentation come from actual project, located in parent directory.

### Key Constants
- **ROMAN_GSUB_PATTERNS**: 65+ pattern replacement rules (lines 9-66)
- **KANJI_GSUB_PATTERNS**: Kanji-side pattern rules (lines 67-79)
- **ONE_KANJI_WORDS**: Special single-kanji handling (lines 81-85)
- **SP_MIRRORS**: Special mirror handling (lines 87-89)

### Core Methods
1. **`roman_kanji_array`** (lines 146-176)
   - Main alignment logic
   - Handles prefix hyphens
   - Handles double hyphens

2. **`splitted_roman`** (lines 115-117)
   - Splits POJ using space
   - **Key**: `split(/\s/)`, not splitting by hyphen

3. **`splitted_kanji`** (lines 119-123)
   - Splits Kanji using RXP_SPK regex
   - Combines with `ONE_KANJI_WORDS` handling

4. **`washed_roman`** (lines 101-106)
   - Applies all ROMAN_GSUB_PATTERNS
   - Normalization processing

5. **`set_arrays`** (lines 134-144)
   - Sets arrays and validates balance
   - Error handling

## Technical Terminology Reference (For Presentation)

| Chinese | Japanese | English | Notes |
|---------|----------|---------|-------|
| åˆ†è© | åˆ†è© | Word Segmentation | Now unified as "åˆ†è©ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆå‡¦ç†" |
| ç™½è©±å­— | ç™½è©±å­— | PeÌh-Åe-jÄ« (POJ) | Taiwanese romanization system |
| è²èª¿æ¨™è¨˜ | å£°èª¿è¨˜å· | Tone Marks | Unicode combining characters |
| é€£å­—ç¬¦ | ãƒã‚¤ãƒ•ãƒ³ | Hyphen | - |
| èªé–“åœé “ | èªé–“åœé “ | Inter-word Pause | `--` symbol, similar to Japanese "ã£" |
| å°é½Š | ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ | Alignment | - |
| å¹³è¡¡æ€§æª¢æŸ¥ | ãƒãƒ©ãƒ³ã‚¹æ¤œè¨¼ | Balance Check | `arrays_balanced` |
| å­—å¥è§£æ | å­—å¥è§£æ | Lexical Analysis | Tokenization |
| æ§‹æ–‡è§£æ | æ§‹æ–‡è§£æ | Syntax Analysis | Pattern Matching |
| æ„å‘³è§£æ | æ„å‘³è§£æ | Semantic Analysis | Validation |

## Speaker Notes Guide

Each page contains detailed Speaker Notes between `<!--` and `-->`.

### Speaker Notes Structure
```markdown
<!--
Speaker Notes Content:
- Key points for this page
- Estimated speaking time
- Technical details to emphasize
- Audience interaction points
-->
```

### Page 12a Speaker Notes Example
```markdown
Now, we reach the core section of this presentation.
The first challenge is the complexity of hyphens.

POJ in Taiwanese requires 3 types of hyphen handling.

First, intra-word hyphens.
These should be preserved.
...

Third, prefix hyphen handling.
Let's look at an actual example.
There's a sentence "æ—¥æ™‚æ–·æ–·ä»”".
In POJ, it's written as "jiÌt--sÃ® tuÄn-tuÄn-Ã¡".

This "--" is a special marker representing inter-word pause.
It's similar to the Japanese geminate consonant "ã£".
```

## Maintenance Notes

### 1. Maintain Code Accuracy
- All code examples must match actual project
- Check latest Git commits before updating code
- Should not show non-existent features

### 2. Scale Classes Adjustment
- Try adjusting scale percentage if content overflows
- Avoid below `scale-65` (65%), affects readability
- Consider splitting into multiple slides

### 3. Terminology Consistency
- Use `åˆ†è©ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆå‡¦ç†` instead of `æ‹†å­—ï¼ˆåˆ†è©ï¼‰`
- Keep terminology reference table updated (Chinese, Japanese, English)
- Confirm Japanese correctness before adding new terms

### 4. Example Updates
- Examples must come from real corpus data
- Provide complete input/output examples
- Explain representativeness of examples

## Preview and Export

### Local Preview
```bash
npx @marp-team/marp-cli@latest -s ./
```

Open http://localhost:8080/rubyworld-2025-taigi-parser.md to check

### Checklist
- [ ] All page content doesn't overflow into footer
- [ ] Code syntax highlighting correct
- [ ] Japanese grammar and terminology correct
- [ ] Speaker Notes complete and clear
- [ ] Example data correct
- [ ] Time controlled within 15 minutes
- [ ] Images and logos display correctly

## Related Resources

### Project Links
- **GitLab**: https://git.5xruby.com/naer/naer/
- **Redmine**: https://redmine.5xruby.com/issues/5432
- **GitHub (Public)**: https://github.com/5xruby/naer

### Reference Documents
- Corpus Model: `app/models/corpus.rb`
- CorporaArraySettable: `app/models/concerns/corpora_array_settable.rb`
- Backend Database File Examples: `å¾Œå°è³‡æ–™åº«æª”æ¡ˆç¯„ä¾‹.xlsx`
- Pre-split Examples: `æ‹†å­—å‰çš„è¡¨æ ¼ç¯„ä¾‹.xlsx`
- Split Correction Results: `æ‹†å­—æ ¡æ­£çµæœçš„ç¯„ä¾‹.xlsx`

### Conference Information
- **Conference**: RubyWorld Conference 2025
- **Date**: November 6-7, 2025
- **Location**: Matsue City, Shimane Prefecture
- **Presentation Time**: 15 minutes (pure presentation, no Q&A)
- **Language**: Japanese

## Basic Information
- **Presentation Title**: ã‚³ãƒ¼ãƒ‰ã®ã‚ˆã†ã«å°æ¹¾èªã‚’è§£æï¼šRubyã«ã‚ˆã‚‹ç™½è©±å­—ãƒ­ãƒ¼ãƒå­—ã®3æ®µéšè§£æ
- **Presenter**: Mu-Fan Teng (é„§æ…•å‡¡)
- **Affiliation**: 5xRuby CO., LTD
- **Presentation Time**: 15 minutes (pure presentation, no Q&A)
- **Language**: Japanese
- **Date**: November 6-7, 2025
- **Location**: Matsue City, Shimane Prefecture

## Project Background

### GitLab Project
- **Project URL**: https://git.5xruby.com/naer/naer/
- Please use MCP to reference
- **Key PRs**:
  - PR #30: Initial implementation (basic rules)
  - PR #68: Introduced ROMAN_GSUB_PATTERNS (40+ rules)
  - PR #73: Special case fixes

### Redmine Issue
- **Issue #5432**: https://redmine.5xruby.com/issues/5432
- Please use MCP to reference
- **Content**: Word splitting requirements and examples

### System Overview
- **Name**: Taiwan Taiwanese Corpus Application Search System (NAER)
- **Client**: Ministry of Education / National Academy for Educational Research
- **Scale**: 208 hours of audio data
- **Target Audience**: Elementary and junior high schools across Taiwan for Taiwanese language education

### Original Data Examples

- Backend Database File Examples.xlsx

## Word Splitting Examples (Core Problem)

- Pre-split Table Examples.xlsx
- Split Correction Results Examples.xlsx

### Input Data
```
TA23_43969	ç´²è½ä¾†çœ‹æ–°ç«¹å¸‚æ˜ä»”è¼‰äºŒåå…­è™Ÿçš„å¤©æ°£	suÃ -loÌh lÃ¢i-khuÃ nn Sin-tik-tshÄ« bÃ®n-Ã¡-tsÃ i gÄ«-tsaÌp-laÌk hÅ Ãª thinn-khÃ¬
```

### Expected Output

#### Kanji Array
```
ç´²è½ï½œä¾†çœ‹ï½œæ–°ç«¹å¸‚ï½œæ˜ä»”è¼‰ï½œäºŒåå…­ï½œè™Ÿï½œçš„ï½œå¤©æ°£
```

#### POJ Array
```
suÃ -loÌhï½œlÃ¢i-khuÃ nnï½œSin-tik-tshÄ«ï½œbÃ®n-Ã¡-tsÃ iï½œgÄ«-tsaÌp-laÌkï½œhÅï½œÃªï½œthinn-khÃ¬
```

### Technical Challenges
1. Hyphen handling
2. Tone marks
3. Mixed Japanese handling
4. Numeric phonetic changes
5. Special symbol handling

## Japanese Technical Terminology Reference

| Chinese/English | Japanese | Reading |
|----------------|----------|---------|
| Word Splitting | åˆ†è© | ã¶ã‚“ã— |
| POJ | ç™½è©±å­— | ãƒšãƒ¼ã‚ªãƒ¼ã‚¸ãƒ¼ |
| Tone Marks | å£°èª¿è¨˜å· | ã›ã„ã¡ã‚‡ã†ãã”ã† |
| Hyphen | ãƒã‚¤ãƒ•ãƒ³ | - |
| Alignment | ã‚¢ãƒ©ã‚¤ãƒ¡ãƒ³ãƒˆ | - |
| Complex | ç…©é›‘ | ã¯ã‚“ã–ã¤ |
| Parser | ãƒ‘ãƒ¼ã‚µãƒ¼ | - |
| Lexical Analysis | å­—å¥è§£æ | ã˜ãã‹ã„ã›ã |
| Syntax Analysis | æ§‹æ–‡è§£æ | ã“ã†ã¶ã‚“ã‹ã„ã›ã |
| Semantic Analysis | æ„å‘³è§£æ | ã„ã¿ã‹ã„ã›ã |
| Regular Expression | æ­£è¦è¡¨ç¾ | ã›ã„ãã²ã‚‡ã†ã’ã‚“ |
| Ruby Evangelist | Rubyä¼é“å¸« | Ruby ã§ã‚“ã©ã†ã— |

### Simplified Demo Code
```ruby
# demo.rb
class TaigiParser
  def initialize
    @patterns = load_patterns
  end

  def parse(input)
    puts "=" * 50
    puts "Input: #{input}"
    puts "=" * 50

    # Step 1: Tokenize
    puts "\n[Step 1] Tokenizing..."
    tokens = tokenize(input)
    puts "Tokens: #{tokens.inspect}"

    # Step 2: Align
    puts "\n[Step 2] Aligning..."
    aligned = align(tokens)
    puts "Aligned: #{aligned.inspect}"

    # Step 3: Validate
    puts "\n[Step 3] Validating..."
    result = validate(aligned)
    puts "Result: #{result ? 'âœ“ Success' : 'âœ— Failed'}"

    aligned
  end
end

# Execution example
parser = TaigiParser.new
parser.parse("suÃ -loÌh lÃ¢i-khuÃ nn Sin-tik-tshÄ«")
```

### Running Tests

```bash
# Test sample data (default, ~3,000 entries)
ruby test_parser.rb

# Or specify test file
ruby test_parser.rb test_data/sample_data.json

# Test full 64,554 entries
ruby test_parser.rb test_data/corpora_data_new.json
```

**Test Script Features**:
- Progress bar display (â–ˆ and â–‘ visual effects)
- Real-time percentage updates
- Final statistics
- Error case display (if any)
- Celebration message on 100% success

## Marp Format Notes

### Basic Settings
```yaml
---
marp: true
theme: default
paginate: true
header: 'RubyWorld Conference 2025'
footer: 'Â© 2025 5xRuby'
---
```

### Suggested Styles
```css
/* Custom styles */
section.center {
  text-align: center;
}

.columns {
  display: grid;
  grid-template-columns: 1fr 1fr;
  gap: 1em;
}

pre {
  font-size: 0.8em;
}

table {
  font-size: 0.9em;
}
```

## Visual Material Suggestions

1. **Opening Page**: 5xRuby Logo + Matsue MOU photo
2. **Story Timeline Chart**: 8 losses â†’ No bidders â†’ Win
3. **POJ Comparison Table**: Animation showing Kanji and POJ correspondence
4. **Code Evolution**: Before/After comparison
5. **Parser Analogy Diagram**: Side-by-side flow chart
6. **Results Data**: Infographic showing 208 hours, schools nationwide
7. **QR Code**: GitHub link

## Time Management Reminders

- Average 30-45 seconds per slide
- Reserve buffer time for demo
- Prepare 15-20 slides
- Use syntax highlighting for key code
- Avoid excessive text, use more visuals

## References

1. **CFP Submission Content**: RubyWorld Conference 2025 Call for Speakers response
2. **System Documentation**: National Academy for Educational Research Taiwan Taiwanese Corpus Application Search System Construction Project
3. **Ministry of Education Project**: Minnan Language Audio Corpus Construction Project (2019-2022)
4. **Technical Documentation**: GitLab project documentation and PR records

## Contact Information

- **Email**: ryudo@5xruby.com
- **GitHub**: https://github.com/5xruby
- **Company**: 5xRuby CO., LTD
- **Phone**: +886939090146

---

*This document is for Claude to generate Marp format slides. Please generate corresponding slide content based on the above.*

---

## Parser Development History

### 2025-10-20: RomanParserPure Optimization - str() vs match[] ğŸ¯

#### Problem Discovery
User questioned: "Isn't it strange to use str for left_quote/right_quote? Since they're the same characters, shouldn't it be regexp?"

#### Analysis and Fix
Although U+201C (") and U+201D (") are different Unicode codepoints, in the Parser phase we only need to "identify it as a quote", not distinguish left/right. Using `match[]` better fits the "character class" semantics.

**Before (V3 - using str() chaining)**:
```ruby
rule(:punctuation) do
  str('...') | str('â‹¯â‹¯') | str('â€¦â€¦') |  # Multi-char first
  str(',') | str('.') | str('!') | str('?') |
  str('ï¼Ÿ') | str('ï¼') |
  str(';') | str(':') | str('(') | str(')') |
  str('"') | str("'") |
  str("\u201C") | str("\u201D") | str("\u2018") | str("\u2019") |  # Curly quotes
  str('ã€Œ') | str('ã€') | str('ã€') | str('ã€') |
  str('â”€') | str('â€¦') | str('â‹¯') |
  str('/') | str('~') | str('ã€')
end
```

**After (V4 - using match[] categorization)**:
```ruby
rule(:punctuation) do
  str('...') | str('â‹¯â‹¯') | str('â€¦â€¦') |  # Multi-char first
  match[',.:;()!?ï¼Ÿï¼/~ã€â”€â€¦â‹¯'] |  # Single-char punctuation
  match["\"'\u201C\u201D\u2018\u2019"] |  # Quotes (ASCII and curly)
  match['ã€Œã€ã€ã€']  # CJK quotes
end
```

#### Test Results

| Implementation | 64,554 Full Data | 3,000 Sample | Parse Success Rate |
|----------------|------------------|--------------|-------------------|
| str() chaining (V3) | 64,191/64,554 (99.44%) | 2,983/3,000 | 99.44% |
| **match[] categorization (V4)** | **64,191/64,554 (99.44%)** | **2,987/3,000 (99.57%)** | **99.44%** |

#### Advantages Analysis
1. **Better fits Parslet conventions**: `match[]` is the standard way to match character sets
2. **More concise code**: 14 lines â†’ 6 lines
3. **Clearer semantics**: Better categorization (single-char punctuation, quotes, CJK quotes)
4. **Performance maintained**: Parse success rate maintained at 99.44% (64,191/64,554)
5. **Educational value**: More suitable for RubyWorld Conference 2025 demonstration

#### File Location
- `experimental/roman_parser_pure.rb` - Lines 42-48

---

### 2025-10-19: Presentation Structure Optimization

#### Main Achievements
1. **Identified and removed 2 redundant pages**
   - Original Page 28: "3-Phase Analysis Details" - Duplicates Phase 1/2/3 pages
   - Original Page 29: "Compiler Theory Application" - Only repeats abstract concepts

2. **Added key insight page**
   - New Page 28: "Why Kanji Doesn't Need a Parser?"
   - Explains how POJ Parser's syllable info automatically achieves Kanji alignment
   - Shows natural 1 syllable = 1 Kanji correspondence

3. **Presentation logic optimization**
   - Before: Comparison â†’ Details (duplicate) â†’ Abstract theory â†’ Ruby advantages
   - After: Comparison â†’ **Why Kanji doesn't need Parser** (key insight) â†’ Ruby advantages
   - More coherent logic, reduced repetition, increased depth

#### Key Design Decisions
- **Parser one-way dependency principle**: Complex structure (POJ) parsed first â†’ Simple structure (Kanji) naturally corresponds
- **Syllable counting method**: `"suÃ -loÌh".split('-').size` directly determines how many Kanji to take
- **Compiler theory analogy**: Same principle as Ruby Parser handling complex grammar then building AST

#### Technical Insights
Core value of this revision:
- Not just showing "Parser can apply to NLP" (abstract)
- But deeply explaining "why only one Parser is needed" (concrete insight)
- Embodies compiler design wisdom: find key structure, others naturally correspond

---

### 2025-10-20: RomanParserPure V5 - Prefix Hyphen Handling ğŸš€

#### Problem Analysis

**Parse failure case**: `"(-pha)"` - prefix hyphen in parentheses cannot be parsed

**Root cause**:
- Current `hyphenated_word` rule: `syllable >> (hyphen >> syllable).repeat`
- Requires starting with `syllable`
- `-pha` starts with hyphen, doesn't match rule âœ—

#### Solution Exploration

**Option 1 (âŒ rejected)**: Define `-` as independent token
```ruby
rule(:token) do
  single_hyphen.as(:hyphen) |  # Standalone hyphen
  hyphenated_word.as(:word) |
  # ...
end
```

**Problem**: Would break alignment
```ruby
"(-pha)" â†’ ["(", "-", "pha", ")"]  # 4 tokens
Kanji:   ['ï¼ˆ', 'è„¬', 'ï¼‰']        # 3 chars
âœ— Unbalanced! And "-" has no corresponding Kanji
```

**Option 2 (âœ… adopted)**: Define `prefix_hyphen_word` as new token type

#### V5 Implementation

```ruby
# New rule - prefix hyphen word
rule(:prefix_hyphen_word) do
  single_hyphen >> syllable
end

# Token priority adjustment
rule(:token) do
  prefix_hyphen_word.as(:word) |  # ğŸ†• Priority match (more specific)
  hyphenated_word.as(:word) |
  number.as(:num) |
  punctuation.as(:punct)
end
```

#### Test Results

| Version | Parse Success | Success Rate | Improvement |
|---------|--------------|--------------|-------------|
| V4 | 64,191/64,554 | 99.44% | - |
| **V5** | **64,208/64,554** | **99.46%** | **+17 cases** |

**Parse errors**: 223 â†’ 205 (-18)

#### Key Advantages

1. **Maintains token integrity**:
   ```ruby
   "(-pha)" â†’ ["(", "-pha", ")"]  # âœ“ -pha as single token
   ```

2. **Matches CorporaArraySettable logic** (Lines 154-158):
   ```ruby
   if rword.match?(/^-/) && rword[1..].exclude?('-')
     [rword]  # Prefix hyphen kept unsplit
   end
   ```

3. **Doesn't break Kanji alignment**:
   ```
   Roman: ["(", "-pha", ")"]  â†’ 3 tokens
   Kanji: ['ï¼ˆ', 'è„¬', 'ï¼‰']   â†’ 3 chars
   âœ“ Balanced!
   ```

4. **Fits POJ semantics**:
   - `-pha` is alternative pronunciation marker in parentheses
   - Corresponds to single Kanji "è„¬"
   - Maintains 1:1 correspondence

#### File Location
- `experimental/roman_parser_pure.rb` - Lines 74-79 (definition), 90-94 (usage)

#### Technical Insights

This fix demonstrates important Parser design principles:
- **Priority order matters**: More specific rules come first
- **Semantics-driven design**: Define token types based on language structure
- **Maintain consistency**: Align with original system's processing logic

---

### 2025-10-20: RomanParserPure V6 - Complete Unicode Range Support ğŸŒ

#### Problem Analysis

**Parse failure statistics**: V5 still has 205 failed cases (0.32%)

Through detailed Unicode character analysis, found failed cases contain many unsupported characters:

**Main problem categories**:

1. **CJK brackets and punctuation** (37 cases)
   - ã€ã€‘(U+3010-3011) - not defined as punctuation
   - ã€‚(U+3002) - Ideographic period

2. **Fullwidth ASCII variants** (32 cases)
   - ï¼…(U+FF05), ï¼ˆ(U+FF08), ï¼‰(U+FF09), ï¼(U+FF0D)
   - Defined U+FF01-FF5E but not fully tested

3. **Bopomofo** (35 cases)
   - ã„…ã„†ã„‡ã„ˆ (U+3105-312F) - completely out of support range
   - Ë‡Ë‹ (U+02C7, U+02CB) - Spacing Modifier Letters

4. **Special symbols**
   - â˜ (U+2610) - Ballot box
   - â€§ (U+2027) - Hyphenation point
   - â¿ (U+207F) - Superscript n
   - % (U+0025) - ASCII percent

#### Solution Exploration

**Strategy**: Systematically expand Unicode ranges, not add individual characters

#### V6 Implementation

**1. Expanded letter definition**:

```ruby
# Add modifier letter and superscript support
rule(:modifier_letter) { match['\u02C0-\u02FF'] }  # Ë‡ Ë‹
rule(:superscript) { match['\u2070-\u209F'] }      # â¿

rule(:letter) do
  unicode_letter |
  (ascii_letter >> combining_mark.repeat) |
  ascii_letter |
  modifier_letter |  # ğŸ†• Tone marks
  superscript        # ğŸ†• Superscript chars
end
```

**2. Significantly expanded punctuation definition**:

```ruby
rule(:punctuation) do
  str('...') | str('â‹¯â‹¯') | str('â€¦â€¦') |
  match[',.:;()!?ï¼Ÿï¼/~ã€â”€â€¦â‹¯\u2027%'] |  # ğŸ†• Added % and â€§
  match["\"'\u201C\u201D\u2018\u2019"] |
  match['\u3000-\u303F'] |  # ğŸ†• Full CJK symbols range
  match['\uFF01-\uFF5E'] |  # ğŸ†• Fullwidth ASCII variants
  match['\u2014'] |
  match['\u2600-\u26FF']     # ğŸ†• Miscellaneous symbols (includes â˜)
end
```

**3. Added special token types**:

```ruby
# Bopomofo as independent token
rule(:bopomofo) { match['\u3105-\u312F'].repeat(1) }

# CJK characters as independent token (handles mixed text)
rule(:cjk_char) { match['\u4E00-\u9FFF'] }

# Updated token rule
rule(:token) do
  prefix_hyphen_word.as(:word) |
  hyphenated_word.as(:word) |
  number.as(:num) |
  bopomofo.as(:bopomofo) |  # ğŸ†•
  cjk_char.as(:cjk) |        # ğŸ†•
  punctuation.as(:punct)
end
```

**4. Updated Transform**:

```ruby
class Transform < Parslet::Transform
  rule(word: simple(:w)) { w.to_s }
  rule(num: simple(:n)) { n.to_s }
  rule(punct: simple(:p)) { p.to_s }
  rule(bopomofo: simple(:b)) { b.to_s }  # ğŸ†•
  rule(cjk: simple(:c)) { c.to_s }       # ğŸ†•
end
```

#### Test Results

| Version | Parse Success | Success Rate | Parse Errors | Improvement |
|---------|--------------|--------------|--------------|-------------|
| V5 | 64,208 | 99.46% | 205 (0.32%) | - |
| **V6** | **64,476** | **99.88%** | **78 (0.12%)** | **+268** |

**Detailed statistics**:
- Total test cases: 64,554
- V5 â†’ V6 improvement: +0.42 percentage points
- Parse errors reduced: 205 â†’ 78 (62% reduction)

#### Newly Supported Unicode Ranges

1. **U+02C0-02FF**: Spacing Modifier Letters
   - Ë‡ (U+02C7 - Caron)
   - Ë‹ (U+02CB - Modifier letter grave accent)

2. **U+2070-209F**: Superscripts and Subscripts
   - â¿ (U+207F - Superscript latin small letter n)

3. **U+3000-303F**: CJK Symbols and Punctuation (full range)
   - ã€‚(U+3002 - Ideographic full stop)
   - ã€(U+3001 - Ideographic comma)
   - ã€(U+3010), ã€‘(U+3011) - Black lenticular brackets
   - ã€€(U+3000 - Ideographic space)

4. **U+3105-312F**: Bopomofo (as independent token)
   - ã„…ã„†ã„‡ã„ˆ etc.

5. **U+4E00-9FFF**: CJK Unified Ideographs (as independent token)
   - Handles Kanji in mixed text

6. **U+FF01-FF5E**: Fullwidth ASCII Variants (full testing)
   - ï¼…(U+FF05), ï¼ˆ(U+FF08), ï¼‰(U+FF09), ï¼(U+FF0D), ï¼(U+FF01)

7. **U+2600-26FF**: Miscellaneous Symbols
   - â˜ (U+2610 - Ballot box)

8. **U+0025**: ASCII percent sign (%)

9. **U+2027**: Hyphenation point (â€§)

#### Key Advantages

1. **Complete coverage**:
   - Supports all Unicode characters appearing in corpus
   - Systematic range definitions, not individual characters

2. **Clear architecture**:
   - Unicode ranges categorized by function
   - Special characters as independent token types

3. **Educational value**:
   - Shows how to handle multilingual mixed text
   - Systematic thinking about Unicode ranges

4. **Practicality**:
   - 99.88% success rate near perfect
   - Remaining 78 cases are extreme edge cases

#### File Location

- `experimental/roman_parser_pure.rb` - Lines 29-59 (Letter rules)
- `experimental/roman_parser_pure.rb` - Lines 50-59 (Punctuation rules)
- `experimental/roman_parser_pure.rb` - Lines 94-114 (Token rules)

---

### 2025-10-20: RomanParserPure V7 - Edge Case Breakthrough ğŸ¯

#### Background

After V6 reached 99.88% (64,476/64,554) success rate, 78 failed cases (0.12%) remained. User requested: "Let's try to solve those remaining edge cases"

#### Failed Case Analysis

Detailed analysis of 78 failed cases found clear patterns:

**Main problem categories**:

1. **Double-hyphen after quotes (59 cases - 76%)**
   - Pattern: `"phrase"--word`
   - Example: `"tsiaÌh-kÃ­n lÃ²ng-phuÃ  uÃ¡nn"--ooh!`
   - Issue: Quote followed directly by `--` then word, no space between
   - Cause: Parser treats `"` as independent token, can't handle adjacent `--word`

2. **Underscore placeholders (9 cases - 11.5%)**
   - Pattern: `lÃ¡n_`
   - Meaning: `å’±__` (indicates blank position in text)
   - Issue: `_` doesn't belong to any defined token type
   - Cause: Underscore not included in letter, punctuation or other rules

3. **Other special characters**
   - Angle brackets: `< lÃ¢ng kah sai >` (1 case)
   - Leading spaces in quotes: `" tÄi it kok-bÃ»n "` (5 cases)
   - Special emoticons: `^Q^` (1 case)
   - Zero-width space: U+200B (1 case)
   - Combining character: U+0358 (2 cases)

#### V7 Implementation Strategy

**Design principle**: Target high-frequency patterns (76% + 11.5% = 87.5%) with dedicated rules

**1. Added double-hyphen word rule**:

```ruby
# Double-hyphen word: starts with double hyphen
# Examples: "--ooh!", "--kÃ³ng"
# Used after quotes: "phrase"--word
rule(:double_hyphen_word) do
  double_hyphen >> syllable >>
  (
    single_hyphen >> syllable |  # Can have more syllables
    single_hyphen                 # Can end with hyphen
  ).repeat
end
```

**Key points**:
- Allows `--` as word beginning
- Can be followed by multiple syllables (e.g., `--kuÃ¨-khÃ¬`)
- Can have trailing hyphen

**2. Added underscore placeholder rule**:

```ruby
# Underscore (used as placeholder)
rule(:underscore) { str('_') }

# Underscore placeholder: word with trailing underscore
# Examples: "lÃ¡n_" meaning "å’±__"
rule(:underscore_word) do
  syllable >> underscore
end
```

**Key points**:
- Define `underscore` as basic token
- `underscore_word` handles `word_` form
- Keep as single token, don't split

**3. Added angle brackets support**:

```ruby
rule(:punctuation) do
  str('...') | str('â‹¯â‹¯') | str('â€¦â€¦') |
  match[',.:;()!?ï¼Ÿï¼/~ã€â”€â€¦â‹¯\u2027%<>'] |  # ğŸ†• Added <>
  # ... rest of patterns
end
```

**4. Adjusted token priority**:

```ruby
rule(:token) do
  double_hyphen_word.as(:word) |  # ğŸ†• Most specific - try first
  prefix_hyphen_word.as(:word) |  # More specific
  underscore_word.as(:word) |     # ğŸ†• Specific pattern
  hyphenated_word.as(:word) |     # General pattern
  number.as(:num) |
  bopomofo.as(:bopomofo) |
  cjk_char.as(:cjk) |
  punctuation.as(:punct)
end
```

**Priority logic**:
- `double_hyphen_word` most specific (`--word`) â†’ Try first
- `prefix_hyphen_word` next (`-word`)
- `underscore_word` specific pattern (`word_`)
- `hyphenated_word` most general (`word-word`) â†’ Try last

#### Test Results

**Edge cases test (7/7)**:

```ruby
[1] âœ“ "tsiaÌh-kÃ­n lÃ²ng-phuÃ  uÃ¡nn"--ooh!
[2] âœ“ "uÃ¬ tsit Ãª khong-keh kÃ u hit Ãª khong-keh"--kÃ³ng
[3] âœ“ "tsiÌt-mÃ¡ lÃ¢i tshap kha"--ooh
[4] âœ“ lÃ¡n_
[5] âœ“ TÄi-tÅo-sÃ²o lÃ¡n_
[6] âœ“ " tÄi it kok-bÃ»n "
[7] âœ“ < lÃ¢ng kah sai >
```

**Full dataset test**:

| Metric | V6 | V7 | Improvement |
|--------|----|----|-------------|
| Total | 64,554 | 64,554 | - |
| Parse Success | 64,476 | **64,548** | **+72** |
| Success Rate | 99.88% | **99.99%** | **+0.11%** |
| Parse Errors | 78 | **6** | **-92.3%** |
| Error Rate | 0.12% | **0.01%** | **-91.7%** |

#### Remaining 6 Extreme Edge Cases

**Detailed analysis**:

1. **[96824]** - ASCII hyphen vs Em dash mixing
   - `lÃ¡n Ãª bÃ³-gÃº - TÃ¢i-uÃ¢n-uÄ“`
   - Uses U+002D (hyphen) instead of U+2014 (em dash)
   - Issue: space + single hyphen + space â‰  valid token

2. **[101361]** - Zero-width space (U+200B)
   - `â€‹ TÃ¢i-uÃ¢n gÃ­-giÃ¢n`
   - Invisible character at beginning
   - Issue: Data cleaning problem, beyond Parser capability

3-4. **[101568, 101572]** - Combining character U+0358
   - `khÃ²Í˜` (Combining Dot Above Right)
   - Need to expand `combining_mark` range to U+0358
   - Issue: Currently only supports U+0300-036F

5. **[106448]** - Comma without space after
   - `kuan-tsiÃ²ng,mÄ`
   - Violates POJ standard format (punctuation should have space after)
   - Issue: Data format problem

6. **[116361]** - Special emoticon
   - `^Q^` (smiley face)
   - Not Unicode standard emoji, belongs to ASCII art
   - Issue: Doesn't conform to linguistic standards

**Classification**:
- **Data quality issues** (3): Zero-width space, comma without space, emoticon
- **Format issues** (1): Hyphen vs Em dash mixing
- **Fixable issues** (2): Combining mark range expansion

#### Technical Achievements

**1. Breakthrough success rate**:
- âœ… 99.99% near perfect
- âœ… Error rate down to 0.01%
- âœ… Production-ready

**2. Problem-solving efficiency**:
- Solved 72/78 cases (92.3%)
- Only 3 steps (double-hyphen, underscore, angle brackets)
- Targeted, avoiding over-engineering

**3. Architectural elegance**:
- Clear rule priority order
- Special patterns separated from general patterns
- Easy to understand and maintain

**4. Educational value**:
- Shows how to perform edge case analysis
- Demonstrates 80/20 principle (solving 87.5% high-frequency problems)
- Explains when to stop optimizing (remaining 0.01% are data issues)

#### Key Design Decisions

**Q: Why put `double_hyphen_word` first?**

A: Parslet PEG parser uses "ordered choice":
- `--word` more specific â†’ Match first
- If placed after `hyphenated_word`, `--` would be seen as two `-`
- Specific rules first, avoid wrong matches

**Q: Why not continue with remaining 6 cases?**

A: Remaining cases are:
1. **Data quality issues** (50%) â†’ Should fix at data cleaning stage
2. **Format standard issues** (17%) â†’ Content errors, not Parser responsibility
3. **Fixable but rare** (33%) â†’ Cost-benefit unreasonable

**ROI**:
- V6 â†’ V7: 3 rules solve 72 cases (24:1)
- V7 â†’ V8 (hypothetical): Need 3+ rules for 6 cases (0.5:1)
- Conclusion: 99.99% reached engineering balance point

#### File Location

- `experimental/roman_parser_pure.rb` - Lines 50-51 (Underscore definition)
- `experimental/roman_parser_pure.rb` - Lines 95-110 (New word rules)
- `experimental/roman_parser_pure.rb` - Lines 127-137 (Token priority)
- `test_v7_full.rb` - Full test script
- `test_v7_edge_cases.rb` - Edge case test
- `analyze_remaining_6.rb` - Remaining case analysis

#### Version Comparison Summary

| Version | Main Improvement | Success Rate | Errors |
|---------|-----------------|--------------|--------|
| V5 | Prefix hyphen | 99.46% | 205 |
| V6 | Unicode ranges | 99.88% | 78 |
| **V7** | **Edge cases** | **99.99%** | **6** |

**Total improvement** (V5 â†’ V7):
- Success rate: +0.53%
- Error reduction: 205 â†’ 6 (-97.1%)
- New rules: 3 (double-hyphen, underscore, angle brackets)

---

### 2025-10-20: RomanParserPure V8 - 100% Perfect Parsing ğŸ¯ğŸ†

#### Background and Challenge

After V7 reached 99.99% (64,548/64,554), 6 remaining cases (0.01%) were marked as "data quality issues".

User challenged: **"Is it still possible to handle these last 6? id IN (96824, 101361, 101568, 101572, 106448, 116361)"**

#### Feasibility Analysis

Detailed technical feasibility assessment of 6 cases:

**Case list and analysis**:

1. **[96824]** - `tshui-sak kap thui-kÃ³ng lÃ¡n Ãª bÃ³-gÃº - TÃ¢i-uÃ¢n-uÄ“.`
   - Pattern: `bÃ³-gÃº - TÃ¢i-uÃ¢n-uÄ“` (space-hyphen-space)
   - Issue: Isolated `-` can't match any token rule
   - Feasibility: âœ… Easy - Add `-` to punctuation

2. **[101361]** - `â€‹ TÃ¢i-uÃ¢n gÃ­-giÃ¢n kÃ u-ioÌk Ãª tshiÃ²-khue`
   - Pattern: Zero-width space (U+200B) at beginning
   - Issue: Invisible character, Parser can't handle
   - Feasibility: âœ… Easy - Add U+200B to `space?`

3. **[101568]** - `" tÄi it kok-bÃ»n " sÄ« ... khÃ²Í˜,`
4. **[101572]** - `lÃ³ng tsiÄm-tsiÄm ... khÃ²Í˜,`
   - Pattern: `khÃ²Í˜` contains Combining Dot Above Right (U+0358)
   - Issue: U+0358 not in U+0300-036F range
   - Feasibility: âœ… Easy - Expand combining_mark + fix letter order

5. **[106448]** - `SÃ®-kan ... "Kong-sÄ« + tshit" pÃªnn-tÃ¢i,`
   - Pattern: Contains `+` sign
   - Issue: `+` not defined as punctuation
   - Feasibility: âœ… Easy - Add `+` to punctuation

6. **[116361]** - `tshut-khÃ¬ ... gÄ“-suÌt-ka--neh!^Q^`
   - Pattern: ASCII art `^Q^` smiley
   - Issue: `^` not defined
   - Feasibility: âœ… Easy - Add `^` to punctuation

**Conclusion: All 6 cases are technically feasible!**

#### V8 Implementation

**1. Zero-width space support**:

```ruby
# Add zero-width space definition
rule(:zero_width_space) { match['\u200B'] }

# Include in space? rule
rule(:space?) { (zero_width_space | match['\s']).repeat }
```

**Key points**:
- U+200B is legal Unicode character
- Common in copy-paste text
- Should be treated as type of whitespace

**2. Combining character U+0358 support**:

```ruby
# Expand combining_mark range
rule(:combining_mark) { match['\u0300-\u036F'] | match['\u0358'] }

# Fix letter definition order
rule(:letter) do
  (unicode_letter >> combining_mark.repeat) |  # ğŸ†• Unicode letter can attach combining
  (ascii_letter >> combining_mark.repeat) |
  unicode_letter |
  ascii_letter |
  modifier_letter |
  superscript
end
```

**Technical details**:
- `khÃ²Í˜` = `k` + `h` + `Ã²` (U+00F2) + `Í˜` (U+0358)
- `Ã²` itself is precomposed Unicode letter
- Need to allow `unicode_letter` followed by `combining_mark`
- Originally only supported `ascii_letter` followed by combining

**Debug process**:
```ruby
# Test findings
"khÃ²Í˜"[2] # => "Ã²" (U+00F2) - Unicode letter, not ASCII!
"khÃ²Í˜"[3] # => "Í˜" (U+0358) - Combining mark

# Original letter definition only handled ASCII + combining
rule(:letter) do
  unicode_letter |
  (ascii_letter >> combining_mark.repeat) |  # âœ— "Ã²" is not ASCII
  ascii_letter
end

# Fixed: Unicode letter can also attach combining
rule(:letter) do
  (unicode_letter >> combining_mark.repeat) |  # âœ“ "Ã²" can attach U+0358
  (ascii_letter >> combining_mark.repeat) |
  # ...
end
```

**3. Special punctuation support**:

```ruby
rule(:punctuation) do
  str(' - ') |  # Kept (though won't be matched)
  str('...') | str('â‹¯â‹¯') | str('â€¦â€¦') |
  match[',.:;()!?ï¼Ÿï¼/~ã€â”€â€¦â‹¯\u2027%<>^+-'] |  # ğŸ†• Added ^, +, -
  # ... rest
end
```

**Key decisions**:
- `^` - Common character in ASCII art emoticons
- `+` - Necessary symbol for mathematical expressions
- `-` - Allow isolated hyphen as em dash substitute

**Token priority impact**:
```ruby
rule(:token) do
  double_hyphen_word.as(:word) |  # "--word" first
  prefix_hyphen_word.as(:word) |  # "-word" second
  underscore_word.as(:word) |     # "word_"
  hyphenated_word.as(:word) |     # "word-word"
  number.as(:num) |
  bopomofo.as(:bopomofo) |
  cjk_char.as(:cjk) |
  punctuation.as(:punct)          # Finally try "-" as punctuation
end
```

**Why can `-` exist in both word and punctuation?**
- PEG parser uses "ordered choice"
- Try all word-related rules first
- Only try punctuation if all fail
- Therefore `bÃ³-gÃº` matches `hyphenated_word`
- But ` - ` (isolated) matches `punctuation`

#### Test Results

**6 target cases test (6/6)**:

```
âœ“ [ID: 96824] bÃ³-gÃº - TÃ¢i-uÃ¢n-uÄ“
âœ“ [ID: 101361] â€‹ TÃ¢i-uÃ¢n gÃ­-giÃ¢n
âœ“ [ID: 101568] ... khÃ²Í˜,
âœ“ [ID: 101572] ... khÃ²Í˜,
âœ“ [ID: 106448] "Kong-sÄ« + tshit"
âœ“ [ID: 116361] ... ^Q^
```

**Full dataset test (64,554 entries)**:

| Metric | V7 | V8 | Improvement |
|--------|----|----|-------------|
| Total | 64,554 | 64,554 | - |
| Parse Success | 64,548 | **64,554** | **+6** |
| Success Rate | 99.99% | **100.00%** | **+0.01%** |
| Parse Errors | 6 | **0** | **-100%** |
| Error Rate | 0.01% | **0.00%** | **-100%** |

ğŸ‰ **Achieved 100% Perfect Parsing!**

#### Technical Achievements

**1. Perfect accuracy**:
- âœ… 64,554/64,554 all successful
- âœ… Zero error rate
- âœ… Production-ready perfect Parser

**2. Proved "data quality issues" are actually linguistic phenomena**:
- Zero-width space â†’ Common byproduct of text editing
- U+0358 â†’ Legal Unicode combining character
- Isolated hyphen â†’ Em dash substitute in typing input
- Plus sign â†’ Necessary for mathematical/mixed language expressions
- Caret â†’ ASCII art emoticon

**3. Maintained architectural elegance**:
- Only 3 small changes (zero_width_space, combining U+0358, punctuation expansion)
- Didn't break existing rules
- Clear token priority logic

**4. Educational value**:
- Shows how to turn "impossible" into "perfect"
- Explains PEG parser's ordered choice feature
- Demonstrates importance of Unicode character analysis

#### Key Design Insights

**Q: Why did V7 consider them "data quality issues"?**

A: Prudent engineering judgment:
- V7 already at 99.99%, deemed remaining 0.01% not worth investment
- Initial assessment: abnormal characters (zero-width space, emoticons)
- Aligns with 80/20 principle engineering decision

**Q: What does V8 prove?**

A: Value of perfectionism:
- Every "edge case" has linguistic justification
- Zero-width space not error, text editing reality
- U+0358 not abnormal, legitimate POJ tone marking method
- Isolated hyphen not format issue, colloquial input habit

**Q: What does this mean for RubyWorld Conference 2025 presentation?**

A: Perfect story ending:
- V1 (98.34%) â†’ V8 (100.00%) complete evolution
- Shows how Compiler theory applies to NLP
- Proves Ruby's Parslet gem can achieve perfect parsing
- **"From no bidders to perfect parsing" inspiring story**

#### File Location

- `experimental/roman_parser_pure.rb` - Lines 26-28 (Zero-width space)
- `experimental/roman_parser_pure.rb` - Line 34 (Combining U+0358)
- `experimental/roman_parser_pure.rb` - Lines 39-46 (Letter order fix)
- `experimental/roman_parser_pure.rb` - Line 59 (Punctuation expansion)
- `test_v8_final_6.rb` - 6 target case tests
- `test_v8_full.rb` - Full dataset test
- `analyze_final_6_feasibility.rb` - Feasibility analysis script

#### Version Comparison Summary

| Version | Main Improvement | Success Rate | Errors | Error Rate |
|---------|-----------------|--------------|--------|-----------|
| V5 | Prefix hyphen | 99.46% | 205 | 0.32% |
| V6 | Unicode ranges | 99.88% | 78 | 0.12% |
| V7 | Edge cases | 99.99% | 6 | 0.01% |
| **V8** | **Perfect parsing** | **100.00%** | **0** | **0.00%** |

**Total improvement** (V5 â†’ V8):
- Success rate: +0.54% (99.46% â†’ 100.00%)
- Error reduction: 205 â†’ 0 (-100%)
- New rules: 6 (double-hyphen, underscore, angle brackets, zero-width, combining U+0358, isolated punctuation)

**Development timeline**:
- V5 â†’ V6: Solved 62% errors (205 â†’ 78)
- V6 â†’ V7: Solved 92.3% errors (78 â†’ 6)
- V7 â†’ V8: Solved final 100% errors (6 â†’ 0)

---

**Last Updated:** 2025-10-20
**Theme Version:** 1.2
**Maintainer:** 5xRuby Development Team
**Achievement Unlocked:** ğŸ† 100% Perfect POJ Parser
